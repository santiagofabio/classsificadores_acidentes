 
### 1. Descri√ß√£o do projeto
<p> Quantificar o risco a que estes caminhoneiros est√£o submetidos,
os dados coletados e analisados anteriormente, ser√£o aplicados a uma colet√¢nea de 
algoritmos de classifica√ß√£o, disponibilizados em linguagem Pyhton atrav√©s das 
bibliotecas de aprendizagem de m√°quina, Sklearn, Lightgbm e Xgboost.</p>

### 2. Cat√°logo de dados


|Categoria | tipo |
|----------|------|
|fumante  |  object|
|bebidas | object
|drogas| object
|acidentes|        float64
|trabalho excessivo|    object
|posicao incomoda|       object
|horas dormindas|    float64
|fss|                   float64
|fadiga|               float64
|horas trabalhadas|     float64
|ess |                  float64
| fumante|object
|Idade |object|
|Dogras| object|
</p>
### 3. Explora√ß√£o de dados

![habitos_de_vida](habitos_de_vida.jpg)

![boxplot_dados_quatitativos_points](boxplot_dados_quatitativos_points.jpg)

![boxplot_dados_quatitativos_horas](boxplot_dados_quatitativos_horas.jpg)

### 4. Algoritmos classificadores
 <p> A fim de quantificar o risco a que estes caminhoneiros est√£o submetidos,os dados coletados e analisados anteriormente, ser√£o aplicados a uma colet√¢nea de algoritmos de classifica√ß√£o, disponibilizados em linguagem Pyhton atrav√©s das bibliotecas de aprendizagem de m√°quina, Sklearn, Lightgbm e Xgboost. </p>

#### 4.1 GaussianNB  
 <p> Consiste em um tipo de aprendizagem supervisionada baseada na aplica√ß√£o do Teorema de Bayes, para isto,assume como hip√≥tese a independ√™ncia de probabilidade 
 para o c√°lculo das probabilidades condicionais. (SCIKIT LEARN ORG., 2022). </p>

#### 4.2 KNeighbors
 <p> A classifica√ß√£o baseada em vizinhos √© um tipo de aprendizagem baseada em inst√¢ncias ou aprendizagem n√£o generalizante: ela 
 n√£o tenta construir um modelo interno geral, mas simplesmente armazena inst√¢ncias dos dados de treinamento. A classifica√ß√£o √©  calculada a partir de uma maioria simples de votos dos vizinhos mais pr√≥ximos de cada ponto: (SCIKIT LEARN ORG., 2022)
 </p>

#### 4.3 Decision Tree
 <p> As √Årvores de Decis√£o (DTs) s√£o um m√©todo de aprendizado supervisionado n√£o param√©trico usado para classifica√ß√£o e regress√£o. O objetivo √© criar um modelo que preveja o valor de uma vari√°vel de destino aprendendo regras de decis√£o simples inferidas dos recursos de dados. Uma √°rvore pode ser vista como 
 uma aproxima√ß√£o constante por partes. (SCIKIT LEARN ORG., 2022)
 </p>

#### 4.3 RandomForest
 <p>  O classificador Random Forest √© uma meta estimador que ajusta v√°rios classificadores de √°rvore de decis√£o em v√°rias subamostras do conjunto de dados e usa a m√©dia para melhorar a precis√£o preditiva e controlar o excesso de ajuste: (SCIKIT LEARN ORG., 2022)
 </p>



#### 4.4 SVM 
 <p> A implementa√ß√£o √© baseada em libsvm. O tempo de ajuste escala pelo menos quadraticamente com o n√∫mero de amostras e pode ser impratic√°vel al√©m de dezenas de milhares de amostras. Para grandes conjuntos de dados, considere usar LinearSVC ou SGDClassifier, possivelmente ap√≥s um transformador Nystroem. (SCIKIT LEARN ORG.,2022)
 </p>

## 4.5 LogisticRegression
 <p> A regress√£o log√≠stica √© um caso especial de Modelos Lineares Generalizados com uma distribui√ß√£o condicional binomial/Bernoulli e um link Logit. A sa√≠da num√©rica da  regress√£o log√≠stica, que √© a probabilidade prevista, pode ser usada como um classificador aplicando-lhe um limite (por padr√£o 0,5). 
 √â assim que √© implementado no scikit-learn, ent√£o ele espera um alvo categ√≥rico, tornando a Regress√£o Log√≠stica um classificador. (SCIKIT LEARN ORG, 2022)
 </p>

#### 4.6 LightGBM
 <p>LightGBM usa algoritmos baseados em histogramas que agrupam valores de recursos (atributos) cont√≠nuos em compartimentos discretos. Isso acelera o treinamento e 
 reduz o uso de mem√≥ria. (LIGHTGBM, 2022)
 </p>

#### 4.7 LightGBM
 <p> O XGBoost √© uma biblioteca otimizada de aumento de gradiente distribu√≠da projetada para ser altamente eficiente, flex√≠vel e port√°til. Ele implementa algoritmos de aprendizado de m√°quina sob a estrutura Gradient Boosting. (DMLC XGBOOST, 2022)</p>

### 5. Matriz de confus√£o

 <p> A execu√ß√£o do procedimento metodol√≥gico para o desenvolvimento das aplica√ß√µes em aprendizagem m√°quinas descritas anteriormente, deve ser capaz de viabilizar a obten√ß√£o de resultados preditivos com elevados n√≠veis de 
 acur√°cia, permitindo assim, a correta mensura√ß√£o dos riscos a que os caminhoneiros est√£o submetidos e a atua√ß√£o preventiva de modo a se mitigar os √≠ndices de acidentes, evitando-se assim perdas humanas, materiais,redu√ß√£o dos custos operacionais e at√© mesmo subsidiar discuss√µes a respeito das politicas voltadas a esta categoria.
</p>

<p> Neste sentido, os algoritmos classificadores aqui       empregados, devem ser capazes superar a natureza aleat√≥ria da ocorr√™ncia do sinistro, assim estes devem fornecer predi√ß√µes com valores superiores a 50%, pois assumisse aqui 
que um determinado motorista item iguais chances de se envolver ou n√£o em um acidente. Assim, uma primeira avali√ß√£o na qual os algoritmos s√£o submissos consiste na obten√ß√£o da matriz de confus√£o deste. 
</p>


 

### 6.  Valida√ß√£o cruzada k-fold 
 <p> Para cada um dos algoritmos a estrat√©gia K-fold Cross Validation foi executada 40 vezes, sendo que, em cada uma delas, foi considerada 10 parti√ß√µes distintas da base de dados, obtendo-se em cada uma delas a previs√£o de um caminhoneiro de envolver em um acidente.
 </p>

 ![teste](teste.PNG)

 <p> A colet√¢nea de todas as previs√µes realizadas durante o processo de valida√ß√£o, permite obter a distribui√ß√£o da acur√°cia para cada um dos algoritmos classificadores, 
 comom √© mostrada na imagem a seguir.
 </p>

 ![distribution_classifier](distribution_classifier.jpg)

 <p>  Al√©m disso, estes dados tamb√©m podem ser  estudados atrav√©s da ferramenta boxplot, mostrada na imagem seguir.
 </p>

 ![boxplot_classidicador](boxplot_classidicador.jpg)
  
 <p>
 Ao analisar conjuntamente as duas imagens anteriores, pode-se concluir que o
 conjunto de algoritmos aqui empregados, atestam como risco de um caminhoneiro se 
 envolver em acidentes probabilidades que variam entre 78% e 85%, o que se mostra 
 significativo como um alerta para a preven√ß√£o deste. Al√©m disso, estes dados revelam a 
 emin√™ncia de um colapso destes trabalhadores, pois estes est√£o assumindo um risco de 
 maior de 78% de sofrerem acidentes ao iniciarem sua jornada de trabalho. </p>

### 7.  Teste de Shapiro-Wilk 
  <p>
   A tabela a seguir re√∫ne os resultados do teste de Shapiro-Wilk (1965) 
 considerando um n√≠vel de confian√ßa de 95%. Assim, conclui-se que as curvas geradas 
 pelos classificadores Naive Bayes, KNN, Random  Forest, SVM, XGBClassifier, s√£o
 normalmente distribu√≠das, enquanto, os classificadores Decision Tree, Logistc Regressor e lightgbm geram curvas que n√£o se distribuem normalmente. Este teste √© importante,pois permite identificar sobre quais curvas devem se aplicar testes param√©tricos e sobre quais iremos aplicar os testes n√£o param√©tricos.
 </p>

  ![tabela_shapiro_wilk](tabela_shapiro_wilk.jpg)
 
 <p>
  Um modo de visualizar a normalidade das curvas dos classificadores, consiste em 
 distribuir os pontos destas sobre a reta QQ-Plot, assim, espera-se que estes se distribuam  sobre elas. As imagens a seguir, mostram retas as QQ-plot, para curvas que passam no  teste de Shapiro-Wilk (1965)
 </p>

  ![NormalQQ_Classifier_linha](NormalQQ_Classifier_linha.jpg)

  ![NormalQQ_Classifier_linha2](NormalQQ_Classifier_linha_p2.jpg)

 #### 7.1 - Teste param√©tricos 

 <p>
 Para o presente estudo, o Teste de hip√≥tese ANOVA e Tukey forma aplicados 
 considerando-se 95% de confian√ßa. O primeiro deles taesta a exist√™ncia de diferen√ßa 
 significativa entre as m√©dias, assim, pode-se ent√£o aplicar o teste de Tukey cujos 
 resultados est√£o elencados na tabela a seguir. a seguir, nesta tabela ùêª0 a hip√≥tese nula,que assume que as medias s√£o iguais, enquanto ùêª1 a hip√≥tese alternativa que considera que os dados s√£o estatisticamente diferentes
 </p>

 #### 7.1.1 - Teste ANOVA
 <p> Ao n√≠vel de confian√ßa de 95% o teste ANOVA garante que existe diferen√ßa estatica entre os dados.


 

 #### 7.1.2 - Teste de Tukey
  teste de Tukey cujos 
 resultados est√£o elencados na tabela a seguir, nesta tabela ùêª0 a hip√≥tese nula,
 que assume que as medias s√£o iguais, enquanto  ùêª1 a hip√≥tese alternativa que considera que os dados s√£o estatisticamente diferentes

 ![tabela_teste_tukey](tabela_teste_tukey.jpg) 


 <p>
 Uma das formas de visualizar as compara√ß√µes realizadas pelo teste de Tukey, √© atrav√©s da m√∫ltipla compara√ß√£o de Tukey, como mostrado a seguir. Desta imagem,podemos concluir que o algoritmo SVM se destaca neste grupo, pois apresenta a maior m√©dia.
 </p>

 ![multiplicomparasion_tukey](multiplicomparasion_tukey.jpg)

#### 7.2 - Teste n√£o param√©tricos
<p>
 Procedendo de maneira an√°loga para os classificadores cujas distribui√ß√µes n√£o 
 foram consideradas normalmente distribu√≠das pelo teste de Shapiro-Wilk, ou seja, os 
 classificadores Decision Tree, Logistc Regressor, lightgbm, deve-se ent√£o aplicar os teste n√£o param√©tricos de Kruskal e Dunn, a fim de compreender o comportamento entre as m√©dias destes classificadores.
</p>
#### 7.2.1 - Teste de  Kruskal
<p>
O teste de hip√≥tese de Kruskal, com devido grau de confian√ßa, assume como 
hip√≥tese nula ùêª0 as m√©dias serem estatisticamente iguais para todas as curvas, quanto que 
a hip√≥tese alternativa ùêª1 assume que existe diferen√ßa estat√≠stica significativa elas. 
Contudo, assim como o teste param√©trico de ANOVA o teste de Kruskal n√£o permite 
identificar onde corre a diferen√ßa, o mapeamento desta fica a cargo do Teste de Hip√≥tese
de Dunn., que com devido n√≠vel de confian√ßa, testa as m√©dias duas a duas, considerando 
como nula ùêª0 que assume igualdade entre elas, e a hip√≥tese alternativa ùêª1 que considera 
a exist√™ncia de diferen√ßa entre elas.
</p>

#### 7.2.2 - Teste de Dunn
 <p>
 Ao n√≠vel de confian√ßa de 95% o teste de hip√≥tese de Kruskal conclui que existe
 diferen√ßa estat√≠stica significativa entre as medias dos classificadores Decision Tree, 
 Logistc Regressor, lightgbm. Assim , aplicando o Teste de hip√≥tese de Dunn com o 
 mesmo n√≠vel de confian√ßa anterior, aos classificadores j√° elencados se obtem a imagem a seguir:
 </p>

 ![multiplicomparasion_dnn](multiplicomparasion_dnn.jpg)